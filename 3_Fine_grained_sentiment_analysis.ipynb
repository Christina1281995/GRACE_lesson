{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/header_absa1.png?raw=true\" width=\"95%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What could be improved in this sentiment analysis?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/limitsofSA.png?raw=true\" width=\"75%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have been doing so far is called \"document-level\" analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/document level.png?raw=true\" width=\"75%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the sentiment result in this tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.7361530661582947}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "classifier(\"This disease #covid19 is REALLY starting to annoy me, but at least the lockdown lets me spend more time with the family which has been amazing! Work-from-home is tough though...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/sentence level.png?raw=true\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9957256317138672}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 1:\n",
    "classifier(\"This disease #covid19 is REALLY starting to annoy me, but at least the lockdown lets me spend more time with the family which has been amazing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.994464099407196}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence 2:\n",
    "classifier(\"Work-from-home is tough though...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the <b>ABSA problem</b>, the concerned target on which the sentiment is expressed shifts from an entire document to an <b>entity or a certain aspect</b> of an entity.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/ABSA.png?raw=true\" width=\"95%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ABSA-problem and tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/an%20absa%20system.png?raw=true\" width=\"50%\" align=\"right\">\n",
    "\n",
    "ABSA encompasses the identification of one or more of <b>four sentiment elements</b>. Depending on the goal researchers set, Zhang et al. (2022) divide their ABSA methods into either <b>Single ABSA</b> tasks (the more conventional method for ABSA, where a method is developed to tackle one single ABSA goal) or <b>Compound ABSA</b> tasks (more recent trends have moved towards developing methods that address two or more sentiment goals in a single method, thereby capturing the dependency between them).\n",
    "\n",
    "- <b>aspect category</b> c defines a unique aspect of an entity and is supposed to fall into a category set C, predefined for each specific domain of interest. For example, ```food``` and ```service``` can be aspect categories for the restaurant domain.\n",
    "\n",
    "- <b>aspect term</b> a is the opinion target which explicitly appears in the given text, e.g., ```“pizza”``` in the sentence “The pizza is delicious.” When the target is implicitly expressed (e.g., “It is overpriced!”), we denote the aspect term as a special one named “null”.\n",
    "\n",
    "- <b>opinion term</b> o is the expression given by the opinion holder to express his/her sentiment towards the target. For instance, ```“delicious”``` is the opinion term in the running example “The pizza is delicious”.\n",
    "\n",
    "- <b>sentiment polarity</b> p describes the orientation of the sentiment over an aspect category or an aspect term, which usually includes ```positive```, ```negative```, and ```neutral```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/e2e.png?raw=true\" width=\"60%\" align=\"center\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we do that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method designed by Luo et al. (2020), described in the paper [GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis. Huaishao Luo, Lei Ji, Tianrui Li, Nan Duan, Daxin Jiang. Findings of EMNLP, 2020.](https://arxiv.org/abs/2009.10557), implements a gradient harmonized and cascaded labeling model.\n",
    "\n",
    "The method falls into the “End 2 End” category of aspect-based sentiment analysis tasks, meaning it solves two ABSA sub-tasks, ATE (asect term extraction) and ASC (aspect semtiment classification), in one model or methodology. Recent advances in the E2E methods leverage the interdependencies between aspect term detection and its sentiment classification to enhance model performances. This stands in contrast to pipeline approaches, which tackle one ABSA sub-task after the other in an isolated manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/grace_structure.png?raw=true\" width=\"90%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/joint.PNG?raw=true\" align=\"right\" width=\"35%\">\n",
    "\n",
    "<u>Key Characteristics:</u>\n",
    "\n",
    "- Co-extraction of ATE and ASC\n",
    "- 2 cascading modules\n",
    "   - 12 stacked transformer encoder blocks for ATE\n",
    "   - 3 shared transformer encoder blocks and 2 transformer decoder blocks for ASC\n",
    "- Focus on interaction\n",
    "- Joint approach\n",
    "- Shared shallow layers (n=3)\n",
    "   - higher layers in BERT are usually task-specific\n",
    "   - it is assumed that can be useful to share the shallow layers\n",
    "   - generates a shared “baseline understanding”\n",
    "\n",
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/grad.PNG?raw=true\" align=\"right\" width=\"35%\">\n",
    "\n",
    "- <b>Virtual adversairal training</b>: the robustness of the model is improved bz preturbing the input data in small ways so that its difficult for the model to classify (to implement this, the direction and distance of the perturbations is calculated)\n",
    "\n",
    "- <b>Gradient harmonized loss</b>: the model is trained with cross entropy loss, but to optimise the model to “focus” more on the “hard” labels, a gradient norm is calculated for each label (where “easy” labels have low gradients) and a weight for the loss calculation is assigned to each label based on the gradient density (histogram statistic). The idea is to decrease the weight of loss form labels with low gradient norms.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Architecture:</u>\n",
    "\n",
    "- <b>Activation</b> function: GeLU (Gaussian Error Linear Unit, non-linear function that maps negative input values to negative outputs and positive input values to positive outputs)\n",
    "- Initial <b>tokenization and embeddings</b> (WordPiece, a subword tokenization method used for the original BERT model)\n",
    "   - A nn.Embeddings layer combines word embeddings, positional embeddings and token type embeddings (n=2)\n",
    "- n x the <b>encoder block</b> (12 in this configuration, same as original BERT model)\n",
    "   - Multi-head Scaled-dot product attention with Softmax to generate context layer\n",
    "   - ‘Intermediate’: linear layer and activation function\n",
    "   - ‘Output’: liner layer, layer normalisation, dropout\n",
    "- The <b>classification head</b> for ATE (nn.Linear, Softmax)\n",
    "- n x the <b>decoder block</b> (2 in this configuration)\n",
    "- The <b>classification head</b> for ASC (nn.Linear, Softmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
